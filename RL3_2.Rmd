---
title: "衡量機器學習模型的表現"
author: "Yao-Jen Kuo"
date: "June, 2016"
output:
  slidy_presentation:
    fig_width: 8
    fig_height: 5
---

## 好的機器學習模型

一個好的機器學習模型大致上具有這幾個特性:

* 準確度高
* 計算時間短
* 容易解釋

## 基本機器學習模型的對應衡量指標

|機器學習模型|衡量表現指標|
|------------|------------|
|分類(Classification)|混淆矩陣Confusion Matrix|
|迴歸(Regression)|均方根誤差RMSE(Root Mean Squared Error)|
|分群(Clustering)|唐恩指數Dunn's Index|

## 混淆矩陣Confusion Matrix

* 以二元分類器為例:

![Source: Google Search](images/confusionMatrix.png)

$$Accuracy = \frac{TP + TN}{TP + FP + FN + TN}$$

$$Precision = \frac{TP}{TP + FP}$$

$$Recall = \frac{TP}{TP + FN}$$

## 混淆矩陣Confusion Matrix - Hands On

* [資料集](https://www.kaggle.com/c/titanic/data)

```{r}
# 載入需要的套件
library(magrittr)
library(rpart)

# 讀取資料
titanic <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL3/data/train.csv", header = TRUE)
titanic <- titanic[, c(2, 3, 5, 6)] %>% na.omit
titanic$Survived <- factor(titanic$Survived, levels = c(1, 0))
str(titanic)

# 建立一個決策樹模型
set.seed(1)
treeModel <- rpart(Survived ~ ., data = titanic, method = "class")
titanicToBePredicted <- titanic[, -1]
prediction <- predict(treeModel, newdata = titanicToBePredicted, type = "class")

# 將混淆矩陣印出來
confusionMatrix <- table(titanic$Survived, prediction, dnn = c("Actual", "Predicted"))
confusionMatrix

# 獲得TP, TN, FP, FN
TP <- confusionMatrix[1, 1]
TN <- confusionMatrix[2, 2]
FP <- confusionMatrix[2, 1]
FN <- confusionMatrix[1, 2]

# 計算Accuracy, Precision, Recall
accuracy <- (TP + TN)/(TP + TN + FP + FN)
precision <- TP/(TP + FP)
recall <- TP/(TP + FN)

# 印出Accuracy, Precision, Recall
accuracy
precision
recall
```

## 混淆矩陣Confusion Matrix - Do It Yourself

* 這次換你囉, 加入`Fare`, `Cabin`, 與`Embarked`這三個變數試試看!

## 均方根誤差RMSE

* 資料點與預測迴歸線的平均距離

$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^N (y_i - \hat y_i)^2}$$

![Source: Google Search](images/rmse.jpg)

## 均方根誤差RMSE - Hands on

```{r}
# 氣溫與冰紅茶銷量
temperature <- c(29, 28, 34, 31, 25, 29, 32, 31, 24, 33, 25, 31, 26, 30)
icedTeaSales <- c(77, 62, 93, 84, 59, 64, 80, 75, 58, 91, 51, 73, 65, 84)
icedTeaData <- data.frame(temperature, icedTeaSales)

# 建立一個線性迴歸模型
lmFit <- lm(formula = icedTeaSales ~ temperature, data = icedTeaData)
icedTeaDataToBePredicted <- data.frame(icedTeaData[, -2])
prediction <- predict(lmFit, newdata = icedTeaDataToBePredicted)
RMSE <- sqrt(sum( (icedTeaData$icedTeaSales - prediction) ^ 2) / nrow(icedTeaData))
RMSE
```

## 均方根誤差RMSE - Do It Yourself(1)

* 來這裡下載資料集[airUCI](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise#)
* 只使用`freq`, `angle`, 以及`chLength`這三個變數來預測`dec`
* 計算RMSE1

```{r}
airUCI <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL3/data/airUCI.csv", header = FALSE, col.names = c("freq", "angle", "chLength", "velocity", "thickness", "dec"))
```

## 均方根誤差RMSE - Do It Yourself(2)

* 再加入`velocity`以及`chLength`這兩個變數, 共5個變數來預測`dec`
* 計算RMSE2
* 比較RMSE1與RMSE2

## 唐恩指數Dunn's Index

* 同一個群集中的**相似度**要高
* 不同群集之間的**相似度**要低
* 所以我們使用**唐恩指數**來判別分群模型的表現

$$Dunn's\,Index = \frac {最小的群集間距離}{最大的群集直徑}$$

## 唐恩指數Dunn's Index - Hands on

```{r}
# 看看鳶尾花資料集的結構
str(iris)

# 建立一個分群模型
irisCluster <- iris[-5]
kmeansIris<-kmeans(irisCluster, centers = 3)

# 作圖
plot(formula = Petal.Length ~ Petal.Width, data = irisCluster, col = kmeansIris$cluster, main = "將鳶尾花做分群", xlab = "花瓣寬度", ylab = "花瓣長度", family = "STHeiti")

# 計算WSS與BSS的比例
WSS <- kmeansIris$tot.withinss
BSS <- kmeansIris$betweenss
ratio <- WSS/BSS
ratio
```

## 唐恩指數Dunn's Index - Do It Yourself

* 來這裡下載資料集[seedsUCI](https://archive.ics.uci.edu/ml/datasets/seeds)
* 仿照前例, 計算WSS/BSS

```{r}
seedsUCI <- read.csv("/Users/tkuo/ntu_train/NTUTrainRL3/data/seedsUCI.csv", header = FALSE, col.names = c("area", "perimeter", "compactness", "length", "width", "asymmetry", "grooveLength", "type"))
seedsUCIToBeClustered <- seedsUCI[, -8]
```

## 訓練與測試

## 誤差與變異